# 第7讲 AI伦理研究

## 7.1 人工智能伦理问题

- 人工智能系统正日益代替人类做出各种决策,包括道德决策。如在司法审判、雇佣、贷款评估等
- 对人类社会影响的担忧几乎超过了对以往任何科技。
- 中国的《新一代人工智能发展规划》中,“人工智能伦理”这一词汇出现了15次之多,足见人工智能伦理在人工智能这一学科中的重要地位。

## 7.2 研发和应用中的伦理

- 需要确立研发中的伦理原则并确保予以贯彻
  - 人工智能研发人员需要遵守一些基本的伦理准则,包括有益性、不作恶、包容性的设计、多样性、透明性,以及隐私的保护
  - 需要建立人工智能伦理审查制度,伦理审查应当是跨学科的、多样性的
  - 在创新政策中考虑伦理,即创新政策应当遵守道德标准,将其作为资金支持等政策的先决条件。
- 应用中的伦理限制：探讨人工智能应用的伦理边界问题。
  - “黑箱”的人工智能和算法系统的使用,人工智能战争武器的开发等

### 7.2.1 道德机器

【道德机器的必要性】

人工智能系统以理性代理人的身份进入人类社会,必然需要遵守人类社会的法律、道德等规范和价值,做出合法、合道德的行为。

【道德机器的实现】

1）识别特定社会群体的规范和价值

- 社会和伦理规范往往体现在行为、语言、习俗、文化符号、手工艺品等中,比较难确定。不存在普世的或者共识性的伦理判断，而是依赖于语境或者群体的判断。
- 人工智能系统一般受到多种规范和价值的约束,诸如法律要求、金钱利益、社会和道德价值等,它们彼此之间可能发生冲突。
- 数据或算法歧视问题。

2）将发现并确定的规范和价值嵌入人工智能系统

- **自上而下型**：利用特定的伦理理论进行分析,指导实现该理论的运算法则和子系统的计算需要”的方法。
- **自下而上型**：“拓展式”方法,并指出该方法“重点在于为主体探索行动和学习方面营造一个环境,鼓励其实施道德可嘉型行为”
- 自动驾驶汽车的类似“电车难题”的伦理问题。

3)评估嵌入人工智能系统的规范和价值是否与人类的相符

- 在人类和人工智能之间建立信任涉及两个层面：

  人工智能系统的透明性和可验证性对于建立信任来说是必要的

  监管者连同使用者、开发者、设计者可以一起界定最低程度的价值一致性和相符性标准,以及评估人工智能可信赖性的标准。

- **兼容人类的人工智能的三项原则**:

  一是利他主义,即机器人的唯一目标是最大化人类价值的实现

  二是不确定性,即机器人一开始不确定人类价值是什么

  三是考虑人类,即人类行为提供了关于人类价值的信息,从而帮助机器人确定什么是人类所希望的价值。

### 7.2.2 制定人工智能伦理规范

- 微软公司CEO纳德拉2016年在演讲中提出微软发展人工智能的六大原则,包括人工智能必须是透明性的、在追求效率最大化的同时不损害人类尊严、必须保护隐私、防止产生偏见、向算法问责,等等
- IBM在2017年达沃斯世界经济论坛上公布了其发展人工智能的三个基本原则:不以取代人类为目的、增加透明性,以及提高技能培训和供给。
- 英特尔公司促进创新和开放发展、创造新的就业机会并保护人们的福利、负责任地促进数据获、重新思考隐私等,合伦理地设计和执行配套的可责性原则。
- 欧盟人工智能立法
  - 2016年5月,法律事务委员会发布《就机器人民事法律规则向欧盟委员会提出立法建议的报告草案》
  - 首先,需要人工智能伦理准则。
  - 其次,机器人法律人格。长期来看,欧盟议会呼吁考虑赋予复杂的自主机器人法律地位(电子人, electronic persons)的可能性。

## 7.3 人工智能伦理问题的跨学科研究趋势

- 可以预见,决策让渡将越来越普遍

- 既有的以人类为核心的道德框架和价值体系

  如何适应人工智能社会及人工智能影响,是一个值得深思的问题。

## 7.4 阿西洛马人工智能原则

- 研究问题
- 1研究目的:创造有益而非不受控制的人工智能
- 2研究资金:资助如何有益利用人工智能的研究,法律、伦理、社会研究等
- 3科学政策连接:建设性的对话交流
- 4研究文化:合作、信任、透明
- 5避免竞赛:在安全标准上积极合作
- 6安全:安全可靠性及可验证
- 7失灵透明性:人工智能系统造成的损害具有可追溯性
- 8责任:对高级人工智能系统的道德影响负责
- 9司法透明性:司法决策中使用人工智能系统应提供解释和救济
- 10.价值对接:人工智能系统的目标和行为符合人类价值
- 11人类价值:人工智能系统必须兼容人类尊严、权利、自由、文化多样性
- 12个人隐私:访问、管理、控制个人数据
- 13自由和隐私:将人工智能用于个人数据必须不能剥夺人们实际的或者可感知的自由
- 14.利益共享:尽可能赋能、惠及更多人
- 15繁荣共享:人工智能带来的经济繁荣应惠及全人类
- 16.人类控制:人类应当决定如何以及是否将决策外包给人工智能系统
- 17.非颠覆性：通过控制人工智能系统获得的权力必须尊重、改善人社会的社会和民事程序。
- 18人工智能军备竞赛:应当被禁止
- 19能力警觉:对于未来人工智能能力的上限未达共识
- 20 重要性以适当的注意和资源计划、管理将对人类社会和地球产生重大影响的人工智能系统
- 21风险:适当预防潜在的毁灭或者生存危机
- 22循环的自我提高:具有此能力的人工智能系统必须县有充分的安全
- 23共同利益:开发超级人工智能必须服务于全人类的利益

## 7.5 软件工程师、AI、伦理

- 代码是完成特定任务的计算机程序、软件或技术架构,代码处于信息开发活动的底层,常被视为单纯的技术现象,因此代码常常处于伦理约束的视野之外,成为一个“隐性问题”。
- 信息时代的智能社会中,我们至少面临编程错误、网络安全、不当使用、共享的自主权和社会经济影响五大类风险考验。
- 用技术来规范网络空间人类行为的研究始于20世纪90年代，网络程序和协议等代码是网络空间的架构
- 人类擅长的因果决策模型与机器学习算法的模型在底层逻辑上并不相同。
- 监控型人工智能系统或算法的道德问题研究
- 算法时代我们将面临诸如知识的不确定性、决策的不可理解性、证据的误导性和算法歧视等一系列问题
- 罗杰森( Simon Rogerson)通过分析结构化项目管理(SPM)的10个步骤, 在软件开发中认真贯彻“尊重、诚实、无偏见、专业胜任、适当关怀、公平、社会成本、效果和效率”等8个伦理原则的方法

【IEEE<<人工智能设计的伦理准则》】

【Ethics\-Aware SE】

【SE 相关的伦理问题】

【在SE中建立与道德价值观和行为的和谐】



